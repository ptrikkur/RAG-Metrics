{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e9b327",
   "metadata": {},
   "source": [
    "# RAG Pipeline with LlamaIndex and Qdrant\n",
    "\n",
    "This notebook demonstrates a complete Retrieval-Augmented Generation (RAG) pipeline using:\n",
    "- **LlamaIndex**: For indexing and retrieval orchestration\n",
    "- **Qdrant**: As the vector database backend\n",
    "- **Reranking**: To improve retrieval quality\n",
    "\n",
    "The pipeline combines document retrieval with LLM-based generation for improved accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd4418",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc3fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_packages():\n",
    "    packages = [\n",
    "        \"llama-index>=0.9.0\",\n",
    "        \"llama-index-vector-stores-qdrant>=0.1.0\",\n",
    "        \"llama-index-embeddings-huggingface>=0.1.0\",\n",
    "        \"qdrant-client>=2.7.0\",\n",
    "        \"sentence-transformers>=2.2.0\",  # For SentenceTransformer reranking\n",
    "        \"python-dotenv>=1.0.0\",\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "# Uncomment to install packages\n",
    "# install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d78bc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poornimata/.pyenv/versions/3.12.2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "from typing import List, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LlamaIndex imports\n",
    "from llama_index.core import Document, VectorStoreIndex, Settings, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "# Qdrant imports\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "# Additional imports\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eec529",
   "metadata": {},
   "source": [
    "## 2. Initialize Qdrant Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08889178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Deleted existing collection: documents\n",
      "INFO:__main__:✓ Created Qdrant collection: documents\n",
      "INFO:__main__:  Vector dimension: 384\n",
      "INFO:__main__:  Distance metric: COSINE\n"
     ]
    }
   ],
   "source": [
    "# Initialize Qdrant Vector Database\n",
    "VECTOR_COLLECTION_NAME = \"documents\"\n",
    "VECTOR_DIMENSION = 384  # Dimension for HuggingFace embeddings\n",
    "\n",
    "# Create an in-memory Qdrant instance (use \":memory:\" for testing)\n",
    "# For production, use: QdrantClient(url=\"http://localhost:6333\")\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "\n",
    "# Recreate collection if it exists\n",
    "try:\n",
    "    qdrant_client.delete_collection(VECTOR_COLLECTION_NAME)\n",
    "    logger.info(f\"Deleted existing collection: {VECTOR_COLLECTION_NAME}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new collection\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=VECTOR_COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(\n",
    "        size=VECTOR_DIMENSION,\n",
    "        distance=Distance.COSINE\n",
    "    ),\n",
    ")\n",
    "\n",
    "logger.info(f\"✓ Created Qdrant collection: {VECTOR_COLLECTION_NAME}\")\n",
    "logger.info(f\"  Vector dimension: {VECTOR_DIMENSION}\")\n",
    "logger.info(f\"  Distance metric: COSINE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac3a231",
   "metadata": {},
   "source": [
    "## 3. Set Up Embeddings and Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0964deb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:1 prompt is loaded, with the key: query\n",
      "INFO:__main__:✓ HuggingFace embedding model loaded\n",
      "INFO:__main__:  Model: BAAI/bge-small-en-v1.5\n",
      "INFO:__main__:✓ LlamaIndex settings configured\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace embeddings\n",
    "# Using sentence-transformers model for semantic embeddings\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    cache_folder=\"./models\"\n",
    ")\n",
    "\n",
    "logger.info(\"✓ HuggingFace embedding model loaded\")\n",
    "logger.info(f\"  Model: BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Configure global settings for LlamaIndex\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "logger.info(\"✓ LlamaIndex settings configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9a3971",
   "metadata": {},
   "source": [
    "## 4. Load and Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab6f08f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✓ Loaded 37 documents from ../docs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully loaded 37 document(s)\n",
      "  Docs folder: /Users/poornimata/Downloads/VectorDB/docs\n",
      "\n",
      "  1. ai_fundamentals.md\n",
      "     Preview: # Artificial Intelligence Fundamentals  Artificial Intelligence (AI) is the simu...\n",
      "\n",
      "  2. deep_learning.md\n",
      "     Preview: # Deep Learning and Neural Networks  Deep Learning uses artificial neural networ...\n",
      "\n",
      "  3. dspy.pdf\n",
      "     Preview: Preprint DSP Y: C OMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF -I MPROVIN...\n",
      "\n",
      "  4. dspy.pdf\n",
      "     Preview: Preprint calls in existing LM pipelines and in popular developer frameworks are ...\n",
      "\n",
      "  5. dspy.pdf\n",
      "     Preview: Preprint 2 R ELATED WORK This work is inspired by the role that Torch (Collobert...\n",
      "\n",
      "  6. dspy.pdf\n",
      "     Preview: Preprint 3.1 N ATURAL LANGUAGE SIGNATURES CAN ABSTRACT PROMPTING & FINETUNING In...\n",
      "\n",
      "  7. dspy.pdf\n",
      "     Preview: Preprint ing Predict to ChainOfThought in the above program leads to a system th...\n",
      "\n",
      "  8. dspy.pdf\n",
      "     Preview: Preprint In DSPy, training sets may be small, potentially a handful of examples,...\n",
      "\n",
      "  9. dspy.pdf\n",
      "     Preview: Preprint ation of DSPy, we focus on demonstrations and find that simple rejectio...\n",
      "\n",
      "  10. dspy.pdf\n",
      "     Preview: Preprint Table 1: Results with in-context learning on GSM8K math word problems. ...\n",
      "\n",
      "  11. dspy.pdf\n",
      "     Preview: Preprint Next, we also consider bootstrapping few-shot examples with random sear...\n",
      "\n",
      "  12. dspy.pdf\n",
      "     Preview: Preprint 7 C ASE STUDY: C OMPLEX QUESTION ANSWERING In this case study, we explo...\n",
      "\n",
      "  13. dspy.pdf\n",
      "     Preview: Preprint Table 2: Results with in-context learning on HotPotQA multi-hop retriev...\n",
      "\n",
      "  14. dspy.pdf\n",
      "     Preview: Preprint discussions and feedback. We thank Giuseppe Attanasio for his public LA...\n",
      "\n",
      "  15. dspy.pdf\n",
      "     Preview: Preprint Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of th...\n",
      "\n",
      "  16. dspy.pdf\n",
      "     Preview: Preprint Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang,...\n",
      "\n",
      "  17. dspy.pdf\n",
      "     Preview: Preprint Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Micha...\n",
      "\n",
      "  18. dspy.pdf\n",
      "     Preview: Preprint Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zho...\n",
      "\n",
      "  19. dspy.pdf\n",
      "     Preview: Preprint A A DVANCED SIGNATURES When more control is desired, one can express si...\n",
      "\n",
      "  20. dspy.pdf\n",
      "     Preview: Preprint icantly lengthy prompt templates, with averages of 1337 and 722 charact...\n",
      "\n",
      "  21. dspy.pdf\n",
      "     Preview: Preprint 1 [web] I will check some things you said. 2 3 (1) You said: Your nose ...\n",
      "\n",
      "  22. dspy.pdf\n",
      "     Preview: Preprint 1 Q: Olivia has $23. She bought five bagels for $3 each. How much money...\n",
      "\n",
      "  23. dspy.pdf\n",
      "     Preview: Preprint 1 2 3 4 def solution(): 5 \"\"\"Jason had 20 lollipops. He gave Denny some...\n",
      "\n",
      "  24. dspy.pdf\n",
      "     Preview: Preprint 1 Solve a question answering task with interleaving Thought, Action, Ob...\n",
      "\n",
      "  25. dspy.pdf\n",
      "     Preview: Preprint 1 Answer the following questions as best you can. You have access to th...\n",
      "\n",
      "  26. dspy.pdf\n",
      "     Preview: Preprint 1 Given the following extracted parts of a long document and a question...\n",
      "\n",
      "  27. dspy.pdf\n",
      "     Preview: Preprint 1 A unity agenda for the nation. 2 We can do this. 3 My fellow American...\n",
      "\n",
      "  28. dspy.pdf\n",
      "     Preview: Preprint 1 A list of documents is shown below. Each document has a number next t...\n",
      "\n",
      "  29. dspy.pdf\n",
      "     Preview: Preprint D M ODULES D.1 P REDICT 1 class Predict(dspy.Module): 2 def __init__(se...\n",
      "\n",
      "  30. dspy.pdf\n",
      "     Preview: Preprint E T ELEPROMPTERS E.1 B OOTSTRAP FEWSHOT 1 class SimplifiedBootstrapFewS...\n",
      "\n",
      "  31. dspy.pdf\n",
      "     Preview: Preprint E.3 B OOTSTRAP FEWSHOTWITH OPTUNA 1 class SimplifiedBootstrapFewShotWit...\n",
      "\n",
      "  32. dspy.pdf\n",
      "     Preview: Preprint F E XAMPLES OF THE PROMPTS AUTOMATICALLY GENERATED BY DSP Y For GSM8K, ...\n",
      "\n",
      "  33. dspy.pdf\n",
      "     Preview: Preprint 1 Given the fields ‘question‘, produce the fields ‘answer‘. 2 3 --- 4 5...\n",
      "\n",
      "  34. dspy.pdf\n",
      "     Preview: Preprint 1 Given the fields ‘context‘, ‘question‘, produce the fields ‘searchque...\n",
      "\n",
      "  35. machine_learning.md\n",
      "     Preview: # Machine Learning Guide  Machine Learning is a subset of AI that focuses on the...\n",
      "\n",
      "  36. nlp.md\n",
      "     Preview: # Natural Language Processing  Natural Language Processing (NLP) enables compute...\n",
      "\n",
      "  37. vector_db_rag.md\n",
      "     Preview: # Vector Databases and RAG  ## Vector Databases  Vector databases like Qdrant ar...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load documents from the docs folder using LlamaIndex\n",
    "docs_path = \"../docs\"\n",
    "\n",
    "# Check if docs folder exists\n",
    "if not os.path.exists(docs_path):\n",
    "    logger.error(f\"Docs folder not found at {docs_path}\")\n",
    "    logger.info(\"Please create a docs folder with markdown or text files\")\n",
    "    documents = []\n",
    "else:\n",
    "    # Use SimpleDirectoryReader to load all documents from the folder\n",
    "    reader = SimpleDirectoryReader(input_dir=docs_path, recursive=True)\n",
    "    documents = reader.load_data()\n",
    "    logger.info(f\"✓ Loaded {len(documents)} documents from {docs_path}\")\n",
    "\n",
    "if documents:\n",
    "    print(f\"\\n✓ Successfully loaded {len(documents)} document(s)\")\n",
    "    print(f\"  Docs folder: {os.path.abspath(docs_path)}\\n\")\n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        title = doc.metadata.get('file_name', 'Unknown')\n",
    "        content_preview = doc.text[:80].replace('\\n', ' ') + \"...\" if len(doc.text) > 80 else doc.text.replace('\\n', ' ')\n",
    "        print(f\"  {i}. {title}\")\n",
    "        print(f\"     Preview: {content_preview}\\n\")\n",
    "else:\n",
    "    print(\"⚠ No documents loaded. Add markdown or text files to the docs folder.\")\n",
    "    print(\"  Example: ../docs/*.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0419ab9",
   "metadata": {},
   "source": [
    "## 5. Create Vector Store Index with LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f602d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✓ QdrantVectorStore created\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sample_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      7\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33m✓ QdrantVectorStore created\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create VectorStoreIndex from the vector store and documents\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# This will automatically generate embeddings and store them in Qdrant\u001b[39;00m\n\u001b[32m     11\u001b[39m vector_index = VectorStoreIndex.from_documents(\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     documents=\u001b[43msample_documents\u001b[49m,\n\u001b[32m     13\u001b[39m     vector_store=vector_store,\n\u001b[32m     14\u001b[39m     embed_model=embed_model,\n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33m✓ VectorStoreIndex created and documents indexed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Total documents indexed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sample_documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'sample_documents' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a QdrantVectorStore instance\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=VECTOR_COLLECTION_NAME\n",
    ")\n",
    "\n",
    "logger.info(\"✓ QdrantVectorStore created\")\n",
    "\n",
    "# Create VectorStoreIndex from the vector store and documents\n",
    "# This will automatically generate embeddings and store them in Qdrant\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "logger.info(\"✓ VectorStoreIndex created and documents indexed\")\n",
    "logger.info(f\"  Total documents indexed: {len(documents)}\")\n",
    "\n",
    "# Display index information\n",
    "print(f\"\\nVector Index Information:\")\n",
    "print(f\"  Collection: {VECTOR_COLLECTION_NAME}\")\n",
    "print(f\"  Indexed Documents: {len(documents)}\")\n",
    "print(f\"  Vector Dimension: {VECTOR_DIMENSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2ebd",
   "metadata": {},
   "source": [
    "## 6. Configure Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SentenceTransformer Reranker\n",
    "# The reranker will improve retrieval quality by reordering results based on semantic relevance\n",
    "# No API key required - runs locally using sentence transformers\n",
    "\n",
    "try:\n",
    "    reranker = SentenceTransformerRerank(\n",
    "        model=\"BAAI/bge-reranker-base\",\n",
    "        top_n=3,  # Keep top 3 results after reranking\n",
    "    )\n",
    "    reranker_available = True\n",
    "    logger.info(\"✓ SentenceTransformer Reranker initialized\")\n",
    "    logger.info(\"  Model: BAAI/bge-reranker-base\")\n",
    "    logger.info(\"  Top N: 3\")\n",
    "    logger.info(\"  Runs locally - no API key required\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not initialize SentenceTransformer Reranker: {e}\")\n",
    "    logger.info(\"✓ RAG pipeline will work without reranking\")\n",
    "    reranker = None\n",
    "    reranker_available = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Reranking Configuration\")\n",
    "print(\"=\"*50)\n",
    "if reranker_available:\n",
    "    print(\"Status: ✓ SentenceTransformer Reranking ENABLED\")\n",
    "    print(\"Model: BAAI/bge-reranker-base\")\n",
    "    print(\"Note: Local reranking - no API key required\")\n",
    "else:\n",
    "    print(\"Status: ⚠ SentenceTransformer Reranking DISABLED\")\n",
    "    print(\"Note: Install: pip install sentence-transformers\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d3f46",
   "metadata": {},
   "source": [
    "## 7. Create RAG Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector index\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=vector_index,\n",
    "    similarity_top_k=5,  # Retrieve top 5 similar documents\n",
    ")\n",
    "\n",
    "logger.info(\"✓ Vector Index Retriever created\")\n",
    "logger.info(\"  Similarity Top K: 5\")\n",
    "\n",
    "# Create the query engine with optional reranking\n",
    "\"\"\" if reranker:\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        node_postprocessors=[reranker],  # Add reranker to improve results\n",
    "    )\n",
    "    logger.info(\"✓ RAG Query Engine created WITH reranking\")\n",
    "else: \"\"\"\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    ")\n",
    "logger.info(\"✓ RAG Query Engine created WITHOUT reranking\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RAG Pipeline Configuration\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Retriever: VectorIndexRetriever\")\n",
    "print(f\"Vector Store: Qdrant (Collection: {VECTOR_COLLECTION_NAME})\")\n",
    "print(f\"Similarity Top K: 5\")\n",
    "#print(f\"Reranker: {'Cohere' if reranker else 'None'}\")\n",
    "print(f\"Embedding Model: BAAI/bge-small-en-v1.5\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d29a1",
   "metadata": {},
   "source": [
    "## 8. Test RAG Pipeline with Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5432fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample queries to test the RAG pipeline\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does deep learning work?\",\n",
    "    \"Tell me about vector databases\",\n",
    "    \"What is RAG and why is it useful?\",\n",
    "]\n",
    "\n",
    "# Test retrieval without generation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING RAG PIPELINE - RETRIEVAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    \n",
    "    # Use retriever to get nodes\n",
    "    retrieved_nodes = retriever.retrieve(query)\n",
    "    \n",
    "    print(f\"Retrieved {len(retrieved_nodes)} documents:\\n\")\n",
    "    for j, node in enumerate(retrieved_nodes, 1):\n",
    "        score = node.score if hasattr(node, 'score') else \"N/A\"\n",
    "        source = node.metadata.get('source', 'Unknown') if hasattr(node, 'metadata') else 'Unknown'\n",
    "        text = node.text[:100] + \"...\" if len(node.text) > 100 else node.text\n",
    "        \n",
    "        print(f\"  [{j}] Source: {source} | Score: {score}\")\n",
    "        print(f\"      Text: {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b5fdf",
   "metadata": {},
   "source": [
    "## 9. Advanced: Creating a Full RAG Pipeline with LLM Generation\n",
    "\n",
    "For complete RAG generation with an LLM, you would integrate with a language model provider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Ollama LLM for RAG pipeline\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Initialize Ollama with llama3.2 1b model\n",
    "llm = Ollama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"llama3.2:1b\",\n",
    "    temperature=0.7,\n",
    "    context_window=2048,\n",
    "    request_timeout=60.0,\n",
    ")\n",
    "\n",
    "logger.info(\"✓ Ollama LLM initialized\")\n",
    "logger.info(\"  Model: llama3.2:1b\")\n",
    "logger.info(\"  Base URL: http://localhost:11434\")\n",
    "logger.info(\"  Temperature: 0.7\")\n",
    "logger.info(\"  Context Window: 2048\")\n",
    "\n",
    "# Set the LLM in global settings\n",
    "Settings.llm = llm\n",
    "\n",
    "# Create the full RAG query engine with Ollama\n",
    "rag_query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[],\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FULL RAG PIPELINE READY WITH OLLAMA LLAMA3.2 1B\")\n",
    "print(\"=\"*70)\n",
    "print(\"Query Engine Configuration:\")\n",
    "print(f\"  LLM: Ollama (llama3.2:1b)\")\n",
    "print(f\"  Retriever: VectorIndexRetriever (top 5)\")\n",
    "#print(f\"  Reranker: {'Cohere' if reranker else 'None'}\")\n",
    "print(f\"  Vector Store: Qdrant\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af62076",
   "metadata": {},
   "source": [
    "## 11. Test Full RAG Pipeline with Ollama LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa64a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test full RAG pipeline with Ollama generation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING FULL RAG PIPELINE WITH OLLAMA GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test queries\n",
    "rag_test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does vector database work?\",\n",
    "    \"Explain RAG in simple terms\",\n",
    "]\n",
    "\n",
    "for i, query in enumerate(rag_test_queries, 1):\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Execute query through full RAG pipeline\n",
    "        response = rag_query_engine.query(query)\n",
    "        \n",
    "        print(f\"\\nGenerated Response:\")\n",
    "        print(f\"{response}\\n\")\n",
    "        \n",
    "        # Display source documents\n",
    "        print(f\"Retrieved Documents Used:\")\n",
    "        if hasattr(response, 'source_nodes') and response.source_nodes:\n",
    "            for j, node in enumerate(response.source_nodes, 1):\n",
    "                source = node.metadata.get('source', 'Unknown') if hasattr(node, 'metadata') else 'Unknown'\n",
    "                score = node.score if hasattr(node, 'score') else \"N/A\"\n",
    "                print(f\"  [{j}] {source} (Score: {score})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing query: {e}\")\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        print(\"\\nNote: Ensure Ollama is running with: ollama serve\")\n",
    "        print(\"And llama3.2:1b model is available: ollama run llama3.2:1b\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RAG pipeline testing complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5504adb",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceb58fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔════════════════════════════════════════════════════════════════════════════╗\n",
    "║                    RAG PIPELINE ARCHITECTURE SUMMARY                       ║\n",
    "╚════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "1. DOCUMENT INGESTION\n",
    "   └─ Load documents from various sources\n",
    "   └─ Split into chunks (512 tokens with 50 token overlap)\n",
    "   └─ Generate embeddings (BAAI/bge-small-en-v1.5)\n",
    "\n",
    "2. VECTOR STORAGE (QDRANT)\n",
    "   └─ Store embeddings in Qdrant vector database\n",
    "   └─ Collection: documents\n",
    "   └─ Vector dimension: 384\n",
    "   └─ Distance metric: Cosine Similarity\n",
    "\n",
    "3. RETRIEVAL\n",
    "   └─ Query engine retrieves top 5 most similar documents\n",
    "   └─ Uses vector similarity search for fast retrieval\n",
    "\n",
    "4. RERANKING (OPTIONAL)\n",
    "   └─ Cohere Rerank models improve relevance ordering\n",
    "   └─ Reduces results to top 3 after reranking\n",
    "   └─ Requires COHERE_API_KEY for API access\n",
    "\n",
    "5. GENERATION (OPTIONAL)\n",
    "   └─ Feed retrieved context to LLM\n",
    "   └─ Supports: OpenAI, Ollama, HuggingFace, etc.\n",
    "   └─ LLM generates answer grounded in retrieved documents\n",
    "\n",
    "KEY FEATURES:\n",
    "✓ No LangChain dependency\n",
    "✓ Pure LlamaIndex implementation\n",
    "✓ Qdrant vector database integration\n",
    "✓ Cohere reranking for improved retrieval\n",
    "✓ Modular and extensible architecture\n",
    "✓ Supports custom embeddings and LLMs\n",
    "\n",
    "NEXT STEPS:\n",
    "1. Load your own documents\n",
    "2. Configure embedding model if needed\n",
    "3. Set COHERE_API_KEY for reranking (optional)\n",
    "4. Integrate an LLM provider for generation\n",
    "5. Customize retrieval parameters for your use case\n",
    "╚════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
