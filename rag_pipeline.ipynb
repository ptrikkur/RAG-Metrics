{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e9b327",
   "metadata": {},
   "source": [
    "# RAG Pipeline with LlamaIndex and Qdrant\n",
    "\n",
    "This notebook demonstrates a complete Retrieval-Augmented Generation (RAG) pipeline using:\n",
    "- **LlamaIndex**: For indexing and retrieval orchestration\n",
    "- **Qdrant**: As the vector database backend\n",
    "- **Reranking**: To improve retrieval quality\n",
    "\n",
    "The pipeline combines document retrieval with LLM-based generation for improved accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd4418",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc3fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_packages():\n",
    "    packages = [\n",
    "        \"llama-index>=0.9.0\",\n",
    "        \"llama-index-vector-stores-qdrant>=0.1.0\",\n",
    "        \"llama-index-embeddings-huggingface>=0.1.0\",\n",
    "        \"qdrant-client>=2.7.0\",\n",
    "        \"sentence-transformers>=2.2.0\",  # For SentenceTransformer reranking\n",
    "        \"python-dotenv>=1.0.0\",\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "# Uncomment to install packages\n",
    "# install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d78bc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poornimata/.pyenv/versions/3.12.2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "from typing import List, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LlamaIndex imports\n",
    "from llama_index.core import Document, VectorStoreIndex, Settings, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "# Qdrant imports\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "# Additional imports\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eec529",
   "metadata": {},
   "source": [
    "## 2. Initialize Qdrant Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08889178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Deleted existing collection: documents\n",
      "INFO:__main__:✓ Created Qdrant collection: documents\n",
      "INFO:__main__:  Vector dimension: 384\n",
      "INFO:__main__:  Distance metric: COSINE\n"
     ]
    }
   ],
   "source": [
    "# Initialize Qdrant Vector Database\n",
    "VECTOR_COLLECTION_NAME = \"documents\"\n",
    "VECTOR_DIMENSION = 384  # Dimension for HuggingFace embeddings\n",
    "\n",
    "# Create an in-memory Qdrant instance (use \":memory:\" for testing)\n",
    "# For production, use: QdrantClient(url=\"http://localhost:6333\")\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "\n",
    "# Recreate collection if it exists\n",
    "try:\n",
    "    qdrant_client.delete_collection(VECTOR_COLLECTION_NAME)\n",
    "    logger.info(f\"Deleted existing collection: {VECTOR_COLLECTION_NAME}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new collection\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=VECTOR_COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(\n",
    "        size=VECTOR_DIMENSION,\n",
    "        distance=Distance.COSINE\n",
    "    ),\n",
    ")\n",
    "\n",
    "logger.info(f\"✓ Created Qdrant collection: {VECTOR_COLLECTION_NAME}\")\n",
    "logger.info(f\"  Vector dimension: {VECTOR_DIMENSION}\")\n",
    "logger.info(f\"  Distance metric: COSINE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac3a231",
   "metadata": {},
   "source": [
    "## 3. Set Up Embeddings and Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0964deb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:1 prompt is loaded, with the key: query\n",
      "INFO:__main__:✓ HuggingFace embedding model loaded\n",
      "INFO:__main__:  Model: BAAI/bge-small-en-v1.5\n",
      "INFO:__main__:✓ LlamaIndex settings configured\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace embeddings\n",
    "# Using sentence-transformers model for semantic embeddings\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    cache_folder=\"./models\"\n",
    ")\n",
    "\n",
    "logger.info(\"✓ HuggingFace embedding model loaded\")\n",
    "logger.info(f\"  Model: BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Configure global settings for LlamaIndex\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "logger.info(\"✓ LlamaIndex settings configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9a3971",
   "metadata": {},
   "source": [
    "## 4. Load and Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab6f08f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✓ Loaded 32 documents from ../docs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully loaded 32 document(s)\n",
      "  Docs folder: /Users/poornimata/Downloads/VectorDB/docs\n",
      "\n",
      "  1. dspy.pdf\n",
      "     Preview: Preprint DSP Y: C OMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF -I MPROVIN...\n",
      "\n",
      "  2. dspy.pdf\n",
      "     Preview: Preprint calls in existing LM pipelines and in popular developer frameworks are ...\n",
      "\n",
      "  3. dspy.pdf\n",
      "     Preview: Preprint 2 R ELATED WORK This work is inspired by the role that Torch (Collobert...\n",
      "\n",
      "  4. dspy.pdf\n",
      "     Preview: Preprint 3.1 N ATURAL LANGUAGE SIGNATURES CAN ABSTRACT PROMPTING & FINETUNING In...\n",
      "\n",
      "  5. dspy.pdf\n",
      "     Preview: Preprint ing Predict to ChainOfThought in the above program leads to a system th...\n",
      "\n",
      "  6. dspy.pdf\n",
      "     Preview: Preprint In DSPy, training sets may be small, potentially a handful of examples,...\n",
      "\n",
      "  7. dspy.pdf\n",
      "     Preview: Preprint ation of DSPy, we focus on demonstrations and find that simple rejectio...\n",
      "\n",
      "  8. dspy.pdf\n",
      "     Preview: Preprint Table 1: Results with in-context learning on GSM8K math word problems. ...\n",
      "\n",
      "  9. dspy.pdf\n",
      "     Preview: Preprint Next, we also consider bootstrapping few-shot examples with random sear...\n",
      "\n",
      "  10. dspy.pdf\n",
      "     Preview: Preprint 7 C ASE STUDY: C OMPLEX QUESTION ANSWERING In this case study, we explo...\n",
      "\n",
      "  11. dspy.pdf\n",
      "     Preview: Preprint Table 2: Results with in-context learning on HotPotQA multi-hop retriev...\n",
      "\n",
      "  12. dspy.pdf\n",
      "     Preview: Preprint discussions and feedback. We thank Giuseppe Attanasio for his public LA...\n",
      "\n",
      "  13. dspy.pdf\n",
      "     Preview: Preprint Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of th...\n",
      "\n",
      "  14. dspy.pdf\n",
      "     Preview: Preprint Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang,...\n",
      "\n",
      "  15. dspy.pdf\n",
      "     Preview: Preprint Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Micha...\n",
      "\n",
      "  16. dspy.pdf\n",
      "     Preview: Preprint Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zho...\n",
      "\n",
      "  17. dspy.pdf\n",
      "     Preview: Preprint A A DVANCED SIGNATURES When more control is desired, one can express si...\n",
      "\n",
      "  18. dspy.pdf\n",
      "     Preview: Preprint icantly lengthy prompt templates, with averages of 1337 and 722 charact...\n",
      "\n",
      "  19. dspy.pdf\n",
      "     Preview: Preprint 1 [web] I will check some things you said. 2 3 (1) You said: Your nose ...\n",
      "\n",
      "  20. dspy.pdf\n",
      "     Preview: Preprint 1 Q: Olivia has $23. She bought five bagels for $3 each. How much money...\n",
      "\n",
      "  21. dspy.pdf\n",
      "     Preview: Preprint 1 2 3 4 def solution(): 5 \"\"\"Jason had 20 lollipops. He gave Denny some...\n",
      "\n",
      "  22. dspy.pdf\n",
      "     Preview: Preprint 1 Solve a question answering task with interleaving Thought, Action, Ob...\n",
      "\n",
      "  23. dspy.pdf\n",
      "     Preview: Preprint 1 Answer the following questions as best you can. You have access to th...\n",
      "\n",
      "  24. dspy.pdf\n",
      "     Preview: Preprint 1 Given the following extracted parts of a long document and a question...\n",
      "\n",
      "  25. dspy.pdf\n",
      "     Preview: Preprint 1 A unity agenda for the nation. 2 We can do this. 3 My fellow American...\n",
      "\n",
      "  26. dspy.pdf\n",
      "     Preview: Preprint 1 A list of documents is shown below. Each document has a number next t...\n",
      "\n",
      "  27. dspy.pdf\n",
      "     Preview: Preprint D M ODULES D.1 P REDICT 1 class Predict(dspy.Module): 2 def __init__(se...\n",
      "\n",
      "  28. dspy.pdf\n",
      "     Preview: Preprint E T ELEPROMPTERS E.1 B OOTSTRAP FEWSHOT 1 class SimplifiedBootstrapFewS...\n",
      "\n",
      "  29. dspy.pdf\n",
      "     Preview: Preprint E.3 B OOTSTRAP FEWSHOTWITH OPTUNA 1 class SimplifiedBootstrapFewShotWit...\n",
      "\n",
      "  30. dspy.pdf\n",
      "     Preview: Preprint F E XAMPLES OF THE PROMPTS AUTOMATICALLY GENERATED BY DSP Y For GSM8K, ...\n",
      "\n",
      "  31. dspy.pdf\n",
      "     Preview: Preprint 1 Given the fields ‘question‘, produce the fields ‘answer‘. 2 3 --- 4 5...\n",
      "\n",
      "  32. dspy.pdf\n",
      "     Preview: Preprint 1 Given the fields ‘context‘, ‘question‘, produce the fields ‘searchque...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load documents from the docs folder using LlamaIndex\n",
    "docs_path = \"../docs\"\n",
    "\n",
    "# Check if docs folder exists\n",
    "if not os.path.exists(docs_path):\n",
    "    logger.error(f\"Docs folder not found at {docs_path}\")\n",
    "    logger.info(\"Please create a docs folder with markdown or text files\")\n",
    "    documents = []\n",
    "else:\n",
    "    # Use SimpleDirectoryReader to load all documents from the folder\n",
    "    reader = SimpleDirectoryReader(input_dir=docs_path, recursive=True)\n",
    "    documents = reader.load_data()\n",
    "    logger.info(f\"✓ Loaded {len(documents)} documents from {docs_path}\")\n",
    "\n",
    "if documents:\n",
    "    print(f\"\\n✓ Successfully loaded {len(documents)} document(s)\")\n",
    "    print(f\"  Docs folder: {os.path.abspath(docs_path)}\\n\")\n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        title = doc.metadata.get('file_name', 'Unknown')\n",
    "        content_preview = doc.text[:80].replace('\\n', ' ') + \"...\" if len(doc.text) > 80 else doc.text.replace('\\n', ' ')\n",
    "        print(f\"  {i}. {title}\")\n",
    "        print(f\"     Preview: {content_preview}\\n\")\n",
    "else:\n",
    "    print(\"⚠ No documents loaded. Add markdown or text files to the docs folder.\")\n",
    "    print(\"  Example: ../docs/*.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9a350a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0419ab9",
   "metadata": {},
   "source": [
    "## 5. Create Vector Store Index with LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f602d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✓ QdrantVectorStore created\n",
      "INFO:__main__:✓ VectorStoreIndex created and documents indexed\n",
      "INFO:__main__:  Total documents indexed: 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vector Index Information:\n",
      "  Collection: documents\n",
      "  Indexed Documents: 32\n",
      "  Vector Dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Create a QdrantVectorStore instance\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=VECTOR_COLLECTION_NAME\n",
    ")\n",
    "\n",
    "logger.info(\"✓ QdrantVectorStore created\")\n",
    "\n",
    "# Create VectorStoreIndex from the vector store and documents\n",
    "# This will automatically generate embeddings and store them in Qdrant\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "logger.info(\"✓ VectorStoreIndex created and documents indexed\")\n",
    "logger.info(f\"  Total documents indexed: {len(documents)}\")\n",
    "\n",
    "# Display index information\n",
    "print(f\"\\nVector Index Information:\")\n",
    "print(f\"  Collection: {VECTOR_COLLECTION_NAME}\")\n",
    "print(f\"  Indexed Documents: {len(documents)}\")\n",
    "print(f\"  Vector Dimension: {VECTOR_DIMENSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2ebd",
   "metadata": {},
   "source": [
    "## 6. Configure Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ae7e4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize SentenceTransformer Reranker\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# The reranker will improve retrieval quality by reordering results based on semantic relevance\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# No API key required - runs locally using sentence transformers\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     reranker = \u001b[43mSentenceTransformerRerank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBAAI/bge-reranker-base\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Keep top 3 results after reranking\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     reranker_available = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     11\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33m✓ SentenceTransformer Reranker initialized\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/llama_index/core/postprocessor/sbert_rerank.py:51\u001b[39m, in \u001b[36mSentenceTransformerRerank.__init__\u001b[39m\u001b[34m(self, top_n, model, device, keep_retrieval_score, trust_remote_code)\u001b[39m\n\u001b[32m     44\u001b[39m device = infer_torch_device() \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m device\n\u001b[32m     45\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     46\u001b[39m     top_n=top_n,\n\u001b[32m     47\u001b[39m     model=model,\n\u001b[32m     48\u001b[39m     device=device,\n\u001b[32m     49\u001b[39m     keep_retrieval_score=keep_retrieval_score,\n\u001b[32m     50\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28mself\u001b[39m._model = \u001b[43mCrossEncoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEFAULT_SENTENCE_TRANSFORMER_MAX_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sentence_transformers/cross_encoder/util.py:39\u001b[39m, in \u001b[36mcross_encoder_init_args_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     37\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mconfig_kwargs\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mclassifier_dropout\u001b[39m\u001b[33m\"\u001b[39m] = classifier_dropout\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:182\u001b[39m, in \u001b[36mCrossEncoder.__init__\u001b[39m\u001b[34m(self, model_name_or_path, num_labels, max_length, activation_fn, device, cache_folder, trust_remote_code, revision, local_files_only, token, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    181\u001b[39m     config.num_labels = num_labels\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mand\u001b[39;00m max_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    195\u001b[39m     tokenizer_kwargs[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_length\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:243\u001b[39m, in \u001b[36mCrossEncoder._load_model\u001b[39m\u001b[34m(self, model_name_or_path, config, backend, **model_kwargs)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_model\u001b[39m(\n\u001b[32m    236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    237\u001b[39m     model_name_or_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m     **model_kwargs,\n\u001b[32m    241\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m         \u001b[38;5;28mself\u001b[39m.model: PreTrainedModel = \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33monnx\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    249\u001b[39m         \u001b[38;5;28mself\u001b[39m.model = load_onnx_model(\n\u001b[32m    250\u001b[39m             model_name_or_path=model_name_or_path,\n\u001b[32m    251\u001b[39m             config=config,\n\u001b[32m    252\u001b[39m             task_name=\u001b[33m\"\u001b[39m\u001b[33msequence-classification\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    253\u001b[39m             **model_kwargs,\n\u001b[32m    254\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:573\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    572\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    577\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    578\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    579\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/transformers/modeling_utils.py:272\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    274\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/transformers/modeling_utils.py:4317\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4312\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4313\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4314\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4315\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4317\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4318\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4319\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4324\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4330\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4333\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4335\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4336\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/transformers/modeling_utils.py:1027\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[39m\n\u001b[32m   1012\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1013\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m   1014\u001b[39m     cached_file_kwargs = {\n\u001b[32m   1015\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcache_dir\u001b[39m\u001b[33m\"\u001b[39m: cache_dir,\n\u001b[32m   1016\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mforce_download\u001b[39m\u001b[33m\"\u001b[39m: force_download,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1025\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m: commit_hash,\n\u001b[32m   1026\u001b[39m     }\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     resolved_archive_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1029\u001b[39m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[32m   1030\u001b[39m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[32m   1031\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[32m   1032\u001b[39m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/transformers/utils/hub.py:266\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    209\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    210\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    211\u001b[39m     **kwargs,\n\u001b[32m    212\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    213\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/transformers/utils/hub.py:424\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    422\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    423\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    439\u001b[39m         snapshot_download(\n\u001b[32m    440\u001b[39m             path_or_repo_id,\n\u001b[32m    441\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    451\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/huggingface_hub/file_download.py:860\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    841\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    842\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m    857\u001b[39m         local_files_only=local_files_only,\n\u001b[32m    858\u001b[39m     )\n\u001b[32m    859\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m860\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m    862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m    864\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/huggingface_hub/file_download.py:1009\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1007\u001b[39m Path(lock_path).parent.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1008\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1019\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1020\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/huggingface_hub/file_download.py:1543\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[39m\n\u001b[32m   1540\u001b[39m         _check_disk_space(expected_size, incomplete_path.parent)\n\u001b[32m   1541\u001b[39m         _check_disk_space(expected_size, destination_path.parent)\n\u001b[32m-> \u001b[39m\u001b[32m1543\u001b[39m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1550\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1552\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1553\u001b[39m _chmod_and_move(incomplete_path, destination_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/huggingface_hub/file_download.py:452\u001b[39m, in \u001b[36mhttp_get\u001b[39m\u001b[34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[39m\n\u001b[32m    450\u001b[39m new_resume_size = resume_size\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[32m    454\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/urllib3/response.py:1091\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1094\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/urllib3/response.py:980\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    977\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    978\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/urllib3/response.py:904\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    901\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    906\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    912\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/urllib3/response.py:887\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    884\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    886\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/http/client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Initialize SentenceTransformer Reranker\n",
    "# The reranker will improve retrieval quality by reordering results based on semantic relevance\n",
    "# No API key required - runs locally using sentence transformers\n",
    "\n",
    "try:\n",
    "    reranker = SentenceTransformerRerank(\n",
    "        model=\"BAAI/bge-reranker-base\",\n",
    "        top_n=3,  # Keep top 3 results after reranking\n",
    "    )\n",
    "    reranker_available = True\n",
    "    logger.info(\"✓ SentenceTransformer Reranker initialized\")\n",
    "    logger.info(\"  Model: BAAI/bge-reranker-base\")\n",
    "    logger.info(\"  Top N: 3\")\n",
    "    logger.info(\"  Runs locally - no API key required\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not initialize SentenceTransformer Reranker: {e}\")\n",
    "    logger.info(\"✓ RAG pipeline will work without reranking\")\n",
    "    reranker = None\n",
    "    reranker_available = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Reranking Configuration\")\n",
    "print(\"=\"*50)\n",
    "if reranker_available:\n",
    "    print(\"Status: ✓ SentenceTransformer Reranking ENABLED\")\n",
    "    print(\"Model: BAAI/bge-reranker-base\")\n",
    "    print(\"Note: Local reranking - no API key required\")\n",
    "else:\n",
    "    print(\"Status: ⚠ SentenceTransformer Reranking DISABLED\")\n",
    "    print(\"Note: Install: pip install sentence-transformers\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d3f46",
   "metadata": {},
   "source": [
    "## 7. Create RAG Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be13b598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✓ Vector Index Retriever created\n",
      "INFO:__main__:  Similarity Top K: 5\n",
      "INFO:__main__:✓ RAG Query Engine created WITHOUT reranking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RAG Pipeline Configuration\n",
      "==================================================\n",
      "Retriever: VectorIndexRetriever\n",
      "Vector Store: Qdrant (Collection: documents)\n",
      "Similarity Top K: 5\n",
      "Embedding Model: BAAI/bge-small-en-v1.5\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever from the vector index\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=vector_index,\n",
    "    similarity_top_k=5,  # Retrieve top 5 similar documents\n",
    ")\n",
    "\n",
    "logger.info(\"✓ Vector Index Retriever created\")\n",
    "logger.info(\"  Similarity Top K: 5\")\n",
    "\n",
    "# Create the query engine with optional reranking\n",
    "\"\"\" if reranker:\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        node_postprocessors=[reranker],  # Add reranker to improve results\n",
    "    )\n",
    "    logger.info(\"✓ RAG Query Engine created WITH reranking\")\n",
    "else: \"\"\"\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    ")\n",
    "logger.info(\"✓ RAG Query Engine created WITHOUT reranking\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RAG Pipeline Configuration\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Retriever: VectorIndexRetriever\")\n",
    "print(f\"Vector Store: Qdrant (Collection: {VECTOR_COLLECTION_NAME})\")\n",
    "print(f\"Similarity Top K: 5\")\n",
    "#print(f\"Reranker: {'Cohere' if reranker else 'None'}\")\n",
    "print(f\"Embedding Model: BAAI/bge-small-en-v1.5\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d29a1",
   "metadata": {},
   "source": [
    "## 8. Test RAG Pipeline with Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5432fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TESTING RAG PIPELINE - RETRIEVAL RESULTS\n",
      "======================================================================\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Query 1: What is machine learning?\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Retrieved 5 documents:\n",
      "\n",
      "  [1] Source: Unknown | Score: 0.6872841694952635\n",
      "      Text: This is inspired by formative work by Bergstra et al.\n",
      "(2010; 2013), Paszke et al. (2019), and Wolf e...\n",
      "\n",
      "  [2] Source: Unknown | Score: 0.6810863004225625\n",
      "      Text: In-context learning methods now routinely invoke tools, leading to LM pipelines that use retrieval\n",
      "m...\n",
      "\n",
      "  [3] Source: Unknown | Score: 0.6770315009422205\n",
      "      Text: Preprint\n",
      "calls in existing LM pipelines and in popular developer frameworks are generally implemente...\n",
      "\n",
      "  [4] Source: Unknown | Score: 0.6687011254713254\n",
      "      Text: Preprint\n",
      "DSP Y: C OMPILING DECLARATIVE LANGUAGE\n",
      "MODEL CALLS INTO SELF -I MPROVING PIPELINES\n",
      "Omar Kha...\n",
      "\n",
      "  [5] Source: Unknown | Score: 0.664805908749748\n",
      "      Text: to logically connect the modules.\n",
      "We then develop theDSPy compiler (Sec 4), which optimizes any DSPy...\n",
      "\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Query 2: How does deep learning work?\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Retrieved 5 documents:\n",
      "\n",
      "  [1] Source: Unknown | Score: 0.7093128870655984\n",
      "      Text: Preprint\n",
      "2 R ELATED WORK\n",
      "This work is inspired by the role that Torch (Collobert et al., 2002), Thea...\n",
      "\n",
      "  [2] Source: Unknown | Score: 0.6870298356372089\n",
      "      Text: 3567–3575. Curran Associates, Inc., 2016. URL https://papers.nips.cc/paper/\n",
      "6523-data-programming-cr...\n",
      "\n",
      "  [3] Source: Unknown | Score: 0.6795558373453233\n",
      "      Text: Preprint\n",
      "calls in existing LM pipelines and in popular developer frameworks are generally implemente...\n",
      "\n",
      "  [4] Source: Unknown | Score: 0.6785138407223708\n",
      "      Text: We conduct\n",
      "two case studies, showing that succinct DSPy programs can express and optimize\n",
      "sophistica...\n",
      "\n",
      "  [5] Source: Unknown | Score: 0.6774623452588889\n",
      "      Text: In-context learning methods now routinely invoke tools, leading to LM pipelines that use retrieval\n",
      "m...\n",
      "\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Query 3: Tell me about vector databases\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Retrieved 5 documents:\n",
      "\n",
      "  [1] Source: Unknown | Score: 0.7317038915362944\n",
      "      Text: 6 Never query for all columns from a table. You must query only the columns that are needed to answe...\n",
      "\n",
      "  [2] Source: Unknown | Score: 0.7282223591854934\n",
      "      Text: Preprint\n",
      "1 A unity agenda for the nation.\n",
      "2 We can do this.\n",
      "3 My fellow Americans|tonight , we have ...\n",
      "\n",
      "  [3] Source: Unknown | Score: 0.7280126506941501\n",
      "      Text: 3567–3575. Curran Associates, Inc., 2016. URL https://papers.nips.cc/paper/\n",
      "6523-data-programming-cr...\n",
      "\n",
      "  [4] Source: Unknown | Score: 0.7248886691892024\n",
      "      Text: Preprint\n",
      "Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic p...\n",
      "\n",
      "  [5] Source: Unknown | Score: 0.7192594344743655\n",
      "      Text: Tools DSPy programs may use tools, which are modules that execute computation. We support re-\n",
      "trieva...\n",
      "\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Query 4: What is RAG and why is it useful?\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Retrieved 5 documents:\n",
      "\n",
      "  [1] Source: Unknown | Score: 0.64332262324024\n",
      "      Text: Preprint\n",
      "1 Answer the following questions as best you can. You have access to the following tools:\n",
      "2...\n",
      "\n",
      "  [2] Source: Unknown | Score: 0.6393220082784472\n",
      "      Text: Tools DSPy programs may use tools, which are modules that execute computation. We support re-\n",
      "trieva...\n",
      "\n",
      "  [3] Source: Unknown | Score: 0.6244126826831767\n",
      "      Text: 1 class BasicMultiHop(dspy.Module):\n",
      "2 def __init__(self, passages_per_hop):\n",
      "3 self.retrieve = dspy.R...\n",
      "\n",
      "  [4] Source: Unknown | Score: 0.6172919155283405\n",
      "      Text: 5 context_match = any((pred.answer.lower() in c) for c in pred.context)\n",
      "6\n",
      "7 return answer_match and ...\n",
      "\n",
      "  [5] Source: Unknown | Score: 0.6133588777940305\n",
      "      Text: Preprint\n",
      "7 C ASE STUDY: C OMPLEX QUESTION ANSWERING\n",
      "In this case study, we explore the multi-hop que...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define sample queries to test the RAG pipeline\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does deep learning work?\",\n",
    "    \"Tell me about vector databases\",\n",
    "    \"What is RAG and why is it useful?\",\n",
    "]\n",
    "\n",
    "# Test retrieval without generation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING RAG PIPELINE - RETRIEVAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    \n",
    "    # Use retriever to get nodes\n",
    "    retrieved_nodes = retriever.retrieve(query)\n",
    "    \n",
    "    print(f\"Retrieved {len(retrieved_nodes)} documents:\\n\")\n",
    "    for j, node in enumerate(retrieved_nodes, 1):\n",
    "        score = node.score if hasattr(node, 'score') else \"N/A\"\n",
    "        source = node.metadata.get('source', 'Unknown') if hasattr(node, 'metadata') else 'Unknown'\n",
    "        text = node.text[:100] + \"...\" if len(node.text) > 100 else node.text\n",
    "        \n",
    "        print(f\"  [{j}] Source: {source} | Score: {score}\")\n",
    "        print(f\"      Text: {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b5fdf",
   "metadata": {},
   "source": [
    "## 9. Advanced: Creating a Full RAG Pipeline with LLM Generation\n",
    "\n",
    "For complete RAG generation with an LLM, you would integrate with a language model provider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8352b246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✓ Ollama LLM initialized\n",
      "INFO:__main__:  Model: llama3.2:1b\n",
      "INFO:__main__:  Base URL: http://localhost:11434\n",
      "INFO:__main__:  Temperature: 0.7\n",
      "INFO:__main__:  Context Window: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FULL RAG PIPELINE READY WITH OLLAMA LLAMA3.2 1B\n",
      "======================================================================\n",
      "Query Engine Configuration:\n",
      "  LLM: Ollama (llama3.2:1b)\n",
      "  Retriever: VectorIndexRetriever (top 5)\n",
      "  Vector Store: Qdrant\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure Ollama LLM for RAG pipeline\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Initialize Ollama with llama3.2 1b model\n",
    "llm = Ollama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"llama3.2:1b\",\n",
    "    temperature=0.7,\n",
    "    context_window=2048,\n",
    "    request_timeout=60.0,\n",
    ")\n",
    "\n",
    "logger.info(\"✓ Ollama LLM initialized\")\n",
    "logger.info(\"  Model: llama3.2:1b\")\n",
    "logger.info(\"  Base URL: http://localhost:11434\")\n",
    "logger.info(\"  Temperature: 0.7\")\n",
    "logger.info(\"  Context Window: 2048\")\n",
    "\n",
    "# Set the LLM in global settings\n",
    "Settings.llm = llm\n",
    "\n",
    "# Create the full RAG query engine with Ollama\n",
    "rag_query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[],\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FULL RAG PIPELINE READY WITH OLLAMA LLAMA3.2 1B\")\n",
    "print(\"=\"*70)\n",
    "print(\"Query Engine Configuration:\")\n",
    "print(f\"  LLM: Ollama (llama3.2:1b)\")\n",
    "print(f\"  Retriever: VectorIndexRetriever (top 5)\")\n",
    "#print(f\"  Reranker: {'Cohere' if reranker else 'None'}\")\n",
    "print(f\"  Vector Store: Qdrant\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af62076",
   "metadata": {},
   "source": [
    "## 11. Test Full RAG Pipeline with Ollama LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaa64a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TESTING FULL RAG PIPELINE WITH OLLAMA GENERATION\n",
      "======================================================================\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Query 1: What is machine learning?\n",
      "──────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "Machine learning is a broad field that encompasses various techniques for developing intelligent systems capable of learning and adapting to new information. At its core, machine learning involves algorithms and statistical models that enable machines to make predictions or take actions based on data and patterns learned from experience. This includes the use of language models, which are computer programs designed to understand, interpret, and generate human language.\n",
      "\n",
      "Retrieved Documents Used:\n",
      "  [1] Unknown (Score: 0.6872841694952635)\n",
      "  [2] Unknown (Score: 0.6810863004225625)\n",
      "  [3] Unknown (Score: 0.6770315009422205)\n",
      "  [4] Unknown (Score: 0.6687011254713254)\n",
      "  [5] Unknown (Score: 0.664805908749748)\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Query 2: How does vector database work?\n",
      "──────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "The general operation of a vector database typically involves storing and manipulating vectors, which are mathematical representations of data points in a high-dimensional space. Vectors can be thought of as numerical summaries or descriptors of the underlying data, allowing for efficient querying and retrieval of specific vectors.\n",
      "\n",
      "Vector databases often employ techniques such as dimensionality reduction, indexing, and similarity searching to facilitate fast and accurate operations on vector data. This enables applications like image recognition, natural language processing, and recommendation systems to leverage vector representations of words, images, or other types of data in a scalable and efficient manner.\n",
      "\n",
      "Retrieved Documents Used:\n",
      "  [1] Unknown (Score: 0.7472497618909842)\n",
      "  [2] Unknown (Score: 0.7251891859935607)\n",
      "  [3] Unknown (Score: 0.7166444358715993)\n",
      "  [4] Unknown (Score: 0.7067593485576941)\n",
      "  [5] Unknown (Score: 0.7022880451162229)\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Query 3: Explain RAG in simple terms\n",
      "──────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "RAG (Retrieval-Augmented Generation) is a technique used to enhance computer systems' performance, particularly in question answering or text generation tasks. It combines input from external knowledge sources with the output of another system to create a more comprehensive understanding of the topic at hand.\n",
      "\n",
      "External knowledge sources are utilized to retrieve relevant information, which is then analyzed and integrated into the generated response by the model like DSPy program. This process improves accuracy and relevance of responses, allowing RAG systems to produce more precise answers in various domains.\n",
      "\n",
      "Retrieved Documents Used:\n",
      "  [1] Unknown (Score: 0.653691818245061)\n",
      "  [2] Unknown (Score: 0.6235403138668445)\n",
      "  [3] Unknown (Score: 0.6229857847746048)\n",
      "  [4] Unknown (Score: 0.6225677549925328)\n",
      "  [5] Unknown (Score: 0.6191630385709631)\n",
      "\n",
      "======================================================================\n",
      "RAG pipeline testing complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test full RAG pipeline with Ollama generation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING FULL RAG PIPELINE WITH OLLAMA GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test queries\n",
    "rag_test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does vector database work?\",\n",
    "    \"Explain RAG in simple terms\",\n",
    "]\n",
    "\n",
    "for i, query in enumerate(rag_test_queries, 1):\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Execute query through full RAG pipeline\n",
    "        response = rag_query_engine.query(query)\n",
    "        \n",
    "        print(f\"\\nGenerated Response:\")\n",
    "        print(f\"{response}\\n\")\n",
    "        \n",
    "        # Display source documents\n",
    "        print(f\"Retrieved Documents Used:\")\n",
    "        if hasattr(response, 'source_nodes') and response.source_nodes:\n",
    "            for j, node in enumerate(response.source_nodes, 1):\n",
    "                source = node.metadata.get('source', 'Unknown') if hasattr(node, 'metadata') else 'Unknown'\n",
    "                score = node.score if hasattr(node, 'score') else \"N/A\"\n",
    "                print(f\"  [{j}] {source} (Score: {score})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing query: {e}\")\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        print(\"\\nNote: Ensure Ollama is running with: ollama serve\")\n",
    "        print(\"And llama3.2:1b model is available: ollama run llama3.2:1b\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RAG pipeline testing complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5504adb",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceb58fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔════════════════════════════════════════════════════════════════════════════╗\n",
    "║                    RAG PIPELINE ARCHITECTURE SUMMARY                       ║\n",
    "╚════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "1. DOCUMENT INGESTION\n",
    "   └─ Load documents from various sources\n",
    "   └─ Split into chunks (512 tokens with 50 token overlap)\n",
    "   └─ Generate embeddings (BAAI/bge-small-en-v1.5)\n",
    "\n",
    "2. VECTOR STORAGE (QDRANT)\n",
    "   └─ Store embeddings in Qdrant vector database\n",
    "   └─ Collection: documents\n",
    "   └─ Vector dimension: 384\n",
    "   └─ Distance metric: Cosine Similarity\n",
    "\n",
    "3. RETRIEVAL\n",
    "   └─ Query engine retrieves top 5 most similar documents\n",
    "   └─ Uses vector similarity search for fast retrieval\n",
    "\n",
    "4. RERANKING (OPTIONAL)\n",
    "   └─ Cohere Rerank models improve relevance ordering\n",
    "   └─ Reduces results to top 3 after reranking\n",
    "   └─ Requires COHERE_API_KEY for API access\n",
    "\n",
    "5. GENERATION (OPTIONAL)\n",
    "   └─ Feed retrieved context to LLM\n",
    "   └─ Supports: OpenAI, Ollama, HuggingFace, etc.\n",
    "   └─ LLM generates answer grounded in retrieved documents\n",
    "\n",
    "KEY FEATURES:\n",
    "✓ No LangChain dependency\n",
    "✓ Pure LlamaIndex implementation\n",
    "✓ Qdrant vector database integration\n",
    "✓ Cohere reranking for improved retrieval\n",
    "✓ Modular and extensible architecture\n",
    "✓ Supports custom embeddings and LLMs\n",
    "\n",
    "NEXT STEPS:\n",
    "1. Load your own documents\n",
    "2. Configure embedding model if needed\n",
    "3. Set COHERE_API_KEY for reranking (optional)\n",
    "4. Integrate an LLM provider for generation\n",
    "5. Customize retrieval parameters for your use case\n",
    "╚════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
